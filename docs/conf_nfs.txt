Network File System (NFS) configuration
=======================================

The most common way to share resources in a cluster is by using a shared network file system (NFS) 
accessible from all the nodes making-up the cluster. 

Configuring a Linux NFS is not a trivial task, in particular when the clients are dynamically created
in a virtual environment. 

*Blow* simplifies this task by only requiring a short declaration in the *cluster* configuration file.


The NFS operation
-----------------

The NFS setup is carried out by the Blow *operation* named ``nfs``. This operation has to be added to
the ``operations`` attribute in the cluster definition. The path to share needs to be specified as a
parameter of the ``nfs`` operation. For example::

  mycluster {
    imageId '..'
    :
    operations {
        nfs(path: '/scratch')
    }
  }


The above declaration will create the ``/scratch`` directory in the master node, start the NFS daemon 
and export that folder to the client nodes. Also it will create the same directory on all the client nodes
and mount the network shared folder.  

It is possible to share more than one NFS folder by simply repeating the ``nfs`` entry as many times as the folders
are to be exported.

For example:: 

  mycluster {
    imageId: ..
    :
    operations {
          nfs(path: '/scratch')
          nfs(path: '/data')
    }

  }


   

Mount and share an EBS volume or snapshots
------------------------------------------

When you start an Amazon EC2 instance, you get a certain amount of storage that depends on the instance type.
This storage is defined as *ephemeral* because it is NOT persisted on the system shut-down. In other words
you lose all your data when the machine is switched off.

To save your data indefinitely you have to use the `Amazon Elastic Block Store`_ (EBS) service. Amazon EBS volumes
provide storage that persists independently from the life of an instance, and can be attached to a running
Amazon EC2 instance and exposed as a Linux device within the instance.

.. _`Amazon Elastic Block Store`: http://aws.amazon.com/ebs/

Once you have created an EBS volume, to get it configured in your cluster all you need to do is to specify its
identifier in the ``supply`` parameter for the ``nfs`` operation. For example::

  mycluster {
    imageId: '..'
    :
    operations {
        nfs(path: '/data', supply: 'vol-123' )
    }

  }  


The above configuration will attach the specified volume to the *master* node, mount it and configure
the NFS sharing. The content of the EBS volume will be accessible on all nodes using the path ``/data``. 

To create a new empty volume, just provide the value ``new`` for the parameter ``supply``.
To specify the size of the new volume, just provide the parameter ``size`` with the number
of GB required. For example::

  mycluster {
    imageId: ..
    :
    operations {
        nfs(path: '/data', supply: 'new', size: 50)
    }

  }  


If the attribute ``size`` is omitted it will create a 10 GB volume by default.

.. attention::
  This creates a new volume each time you launch a new instance.



The new resulting volume, is not deleted on the cluster shut-down, and its content can reused later on the
next restart.

It this is not required, you can delete it on the cluster termination,
adding the parameter ``deleteOnTermination`` as shown below::

  mycluster {
    imageId: '..'
    :
    operations {
        nfs(  path: '/data', supply: 'snap-nnn', size: 50, deleteOnTermination: true )
    }

  }


It is event possible to create, mount and share a new volume with the content provided by snapshot, by simply
providing the identifier of the snapshot in the ``supply`` parameter as explained above.
