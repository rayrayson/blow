Network File System (NFS) configuration
=======================================

The most common way to share resources in a cluster is by using a shared network file system (NFS) 
accessible from all the nodes making-up the cluster. 

Configuring a Linux NFS is not a trivil task, in particular when the clients are dinamically created 
in a virtual environment. 

*Blow* simplifies this task by only requiring a short declaration in the *cluster* configuration file.


The NFS plugin
--------------

The NFS setup is carried out by the *Blow* plugin named ``nfs``. This plugin has to be added to 
the ``plugin`` attribute in the cluster definition and the *shared path* needs to be specified as an 
attribute of the ``nfs`` plugin. For example:: 

  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { path: /scratch  }}
    ]
  }


The above declaration will create the ``/scratch`` directory in the master node, start the NFS daemon 
and export that folder to the client nodes. Also it will create the same directory on all client nodes 
and mount the network shared folder.  

It is possible to share more than one NFS folder by simply repeating the ``nfs`` entry as many times as the folders are to be exported. 
The only difference is that in this case we need to provide the ``device`` name to be used to mount them.

For example:: 

  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { path: /scratch, device: /dev/sdf  }}
      { nfs: { path: /data, device: /dev/sdg  }}
    ]
  }


When you start an Amazon EC2 instance, you get a certain amount of storage that depends by the instance type. 
This storage is defined as *ephemeral* because it is persisted on the system termination. In other words 
you lose all your data when the machine is switched off. 

To save your data indefinitely you have to use the `Amazon Elastic Block Store`_ (EBS) service. Amazon EBS volumes 
provides storage that persists independently from the life of an instance, and can be attached to a running 
Amazon EC2 instance and exposed as a Linux device within the instance. 

.. _`Amazon Elastic Block Store`: http://aws.amazon.com/ebs/
   

Mount and Share an EBS volume
-----------------------------

Once you have created an EBS volume, to get it configured in your cluster all you need to do is to specify the 
property ``volume-id`` in the ``nfs`` configuration. For example::

  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { path: /data, volume-id: vol-123  }}
    ]
  }  


The above configuration will attach the specified volume to the *master* node, mount it and configure
the NFS sharing. The content of the EBS volume will be accessible on all nodes using the path ``/data``. 

To create a new empty volume, just provide the value ``new`` for the attribute ``volume-id``. The new volume
size can be specified providing the attribute ``size`` which specify the number of GB required. For example:: 

  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { path: /data, volume-id: new, size: 50  }}
    ]
  }  


If omitted it will created a 10 GB volume by default. 

.. attention::
  This creates a new volume each time you will launch a new instance. 


Mount Share an EBS snapshot
---------------------------

Amazon EBS also provides the ability to create point-in-time snapshots of volumes, 
which are persisted to Amazon S3. These snapshots can be used as the starting point for 
new Amazon EBS volumes, and protect data for long-term durability.

A common use case is to create a new EBS volume with the content of a previously saved snapshot. 

Using *Blow* this can be done by simply replacing the ``volume-id`` with the attribute ``snapshot-id`` 
in the ``nfs`` plugin configuration, and specifying the ID of a snashot previously created. Optionally 
can be provided the size in GB of the newly create volume.  For example:: 


  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { path: /data, snapshot-id: < the id >, size: 50  }}
    ]
  }  

The new resulting volume, is not deleted on the system termination and its content is preserved across instances restarts.
 
It this is not required, it is posible to specify to delete the newly created volume on the cluster termination, 
adding the attribute ``delete-on-terminate`` as shown below::

  my_cluster {
    image-id: ..
    :
    plugin [ 
      hosts 
      { nfs: { 
           path: /data, 
           snapshot-id: < the id >, 
           size: 50  
           delete-on-termination: true
       }}
    ]
  } 
 
