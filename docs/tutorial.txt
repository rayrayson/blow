Tutorial
========

This short tutorial shows how to configure and run a SGE cluster in the Amazon public cloud using *Blow*.

The cluster is made up of 5 nodes, as showed in the picture below. One node is defined as the SGE *master* node,
and the remaining 4 instances will act as the *worker* nodes in the SGE cluster.

To the *master* node are attached and mounted two EBS volumes. The first contains the application binaries and
is exported via a *Network File System* (NFS) share with the path ``/soft``.

The second volume is used to share applications data and is exported by a NFS share with the path ``/scratch``.

.. image:: images/cluster.png
   :width: 600


It is assumed that you have, at least, a basic knowledge of the *SGE* grid engine workflow. A detailed explanation of *SGE*
concepts and commands is available at this link: http://gridscheduler.sourceforge.net/howto/howto.html



1. Download the *Blow* executable distribution
----------------------------------------------

The latest *Blow* executable distribution is available at `this link`_. Download and
save it somewhere on your computer.

If you are running a \*nix system (Linux, Mac OS X, etc.) grant the execute permission
to the downloaded *JAR* file, with the following command::

  chmod +x blow.run
  
  

.. _`this link`: http://dl.dropbox.com/u/376524/blow/blow.run


2. The configuration file
--------------------------------

Create a file named ``blow.conf`` with the content shown below. Save it in the same directory
where the ``blow.run`` was downloaded. ::


  simplecluster {
      accessKey '<replace with your AWS access key>'
      secretKey '<replace with your AWS secret key>'
      regionId 'us-east-1'
      imageId 'ami-e565ba8c'
      instanceType 't1.micro'
      instanceNum 5

      operations {
          nfs(path: '/soft')
          nfs(path: '/scratch')
          sge(path: '/soft/sge6')
      }
  }


This configuration will deploy a cluster made up of 5 nodes. It uses the Amazon Linux AMI 2012.03 (x86_64) with ID ``ami-e565ba8c``.

It installs the SGE resource manager in the shared path ``/soft/sge6`` and a second shared path named ``/scratch`` is created.


3. Launch the cluster 
---------------------

The *Blow* command needs to be executed using the command line interface. It has the following syntax::

  blow.run [options] [command name [command parameters] ]

If the *command name* is omitted it will run in the *interactive* mode, waiting for commands to be executed.

.. note::
  Windows users need to launch the *Blow* application with the *Java* interpreter. So the above syntax must be replaced 
  as follows::
  
    java -jar blow.run [blow options] [command name [command parameters] ]
     

For the sake of this example start *Blow* in the interactive mode. Open a terminal window, 
move to the directory where the configuration file and the *Blow* executable have been saved and
type the command::

  ./blow.run
  
If a SSH `key pair` cannot be found in the current directory or in the ``$HOME/.ssh/`` path,
it will prompt you to create a new key pair.
Answer ``y`` and press enter to proceed. 

When done you will get the *Blow* prompt as shown below::

     ___  __
    / _ )/ /__ _    __
   / _  / / _ \ |/|/ /
  /____/_/\___/__,__/  ver: 0.7

  blow [simplecluster] $

Before launching the cluster you may verify the configuration by entering the ``conf`` command on the *Blow* prompt.
Something similar to example shown below will appear on your screen::

    blow [simplecluster] $ conf
    cluster: simplecluster {
      accessKey    '...'
      createUser   true
      imageId      'ami-e565ba8c'
      inboundPorts '22,9000'
      instanceNum  5
      instanceType 't1.micro'
      privateKey   './id_rsa'
      publicKey    './id_rsa.pub'
      regionId     'us-east-1'
      secretKey    '...'
      zoneId       'us-east-1a'
      operations {
          hostname()
          nfs( path: '/soft' )
          nfs( path: '/scratch' )
          sge( path: '/soft/sge6' )
      }
    }

When ready, launch the cluster by entering the command ``start``::


  blow [simplecluster] $ start
  
.. warning::
  Your Amazon AWS account will be charged correspondingly the number of instances launched, their configuration and how
  long you will keep them running. For the sake of this tutorial we have used five instances of type ``t1.micro``, which
  costs $0.02 by hour for each instance.

It will take a few minutes to start the virtual machines, configure them and install the SGE daemons.
When it is finished something similar to the following output will appear on your screen::

    blow [simplecluster] $ start
    Please confirm that you want to start 5 node(s) cluster [y/n] y
    Starting cluster: simplecluster
    Creating 1 node for role 'master'
    Creating 4 nodes for role 'worker'
    Configuring cluster hostname(s)
    Configuring NFS file system for shared path '/soft'
    Configuring NFS file system for shared path '/scratch'
    Configuring OpenGridEngine (SGE)


.. attention::
  If the following message is displayed:

  ``WARN: Disabling high-strength ciphers: cipher strengths apparently limited by JCE policy``

  you will need to install the *Java Cryptography Extension (JCE)* for high-strength ciphers.
  This is not included by default due to US export restrictions. This is available in the `Java download`_ at
  the bottom of the page.

.. _`Java download`: http://www.oracle.com/technetwork/java/javase/downloads/index.html


4. Access the remote master node
--------------------------------

Use the command ``listnodes`` on the *Blow* prompt to get the list of all running nodes in the cluster.
A list similar to the one shown below will appear on your screen::

    blow [simplecluster] $ listnodes
    master  (i-8c07fcf6); 184.72.69.208 ; RUNNING
    worker1 (i-e404ff9e); 184.73.98.6   ; RUNNING
    worker2 (i-d804ffa2); 50.17.139.31  ; RUNNING
    worker3 (i-e604ff9c); 23.22.161.189 ; RUNNING
    worker4 (i-da04ffa0); 75.101.191.174; RUNNING


Any node is accessible using an SSH client. *Blow* comes with an integrated SSH client.
To connect to a remote node on the prompt just enter the ``ssh`` command followed by the name
of the node you want to access::

  ssh master

.. attention::
  The integrated SSH client automatically manages the access credentials. Using a different
  SSH client you will need to specify the user name, the identity file (the private key)
  and the remote instance IP address.
  
When connected to the remote node, you may want to verify that the *SGE* scheduler has been 
installed correctly. At the shell prompt of the remote system enter the command ``qhost``.
It should print a list similar to the one shown below::

    [user@master ~]$ qhost
    HOSTNAME                ARCH         NCPU  LOAD  MEMTOT  MEMUSE  SWAPTO  SWAPUS
    -------------------------------------------------------------------------------
    global                  -               -     -       -       -       -       -
    master                  linux-x64       1  0.17  595.4M   58.4M     0.0     0.0
    worker1                 linux-x64       1  0.37  595.4M   48.2M     0.0     0.0
    worker2                 linux-x64       1  0.13  595.4M   48.4M     0.0     0.0
    worker3                 linux-x64       1  0.15  595.4M   48.2M     0.0     0.0
    worker4                 linux-x64       1  0.11  595.4M   48.2M     0.0     0.0
  

  
5. Run a simple job and terminate the cluster  
---------------------------------------------

Move to the ``/scratch`` folder, create a job script and submit it to the *SGE* for execution.
For the sake of this tutorial, you could run these commands::

  cd /scratch 
  echo "echo Hello world!" > test.sh
  qsub -cwd -sync y test.sh 
  

When the job get executed, you will find the following content in the
current directory::

  $ ls -la 
  total 16
  drwxr-xr-x  2 ec2-user wheel 4096 May 11 14:15 .
  dr-xr-xr-x 26 root     root  4096 May 11 13:59 ..
  -rw-r--r--  1 ec2-user wheel   19 May 11 14:14 test.sh
  -rw-r--r--  1 ec2-user wheel    0 May 11 14:15 test.sh.e1
  -rw-r--r--  1 ec2-user wheel   97 May 11 14:15 test.sh.o1

6. Terminate the cluster session
--------------------------------

When you have done this, don't forget to shutdown the cluster along with all the running instances.

Log out from the remote node using the ``exit`` command and stop the cluster, by entering
the command ``terminate`` at the *Blow* prompt.